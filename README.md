This project implements a Convolutional Neural Network (CNN) to classify spoken digits (0â€“9) using spectrograms generated from audio samples. 
The model was trained and validated on a dataset of 2,000 audio samples, achieving a validation accuracy of 99.25%.
Audio Preprocessing:

Audio samples are converted to spectrograms using the Short-Time Fourier Transform (STFT).
The spectrograms are saved as PNG images categorized by their respective digit labels.

Model Training:

A CNN model is trained on the spectrogram dataset.

Evaluation:

The model is validated on a separate validation set.

<img width="1177" height="706" alt="image" src="https://github.com/user-attachments/assets/a27d84de-88c0-499b-9a28-b1e59e2cd8a7" />
